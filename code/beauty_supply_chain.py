# -*- coding: utf-8 -*-
"""Beauty_supply_chain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zT6ccT3Uva9tKAVCY0ewmCmpLhRK9PKV

**Predict product demand by region and optimize inventory levels to minimize stockouts, overstocking, and total supply chain costs.**
"""

import pandas as pd
import numpy as np
from google.colab import files
import io

# STEP 1: Upload file manually through Colab upload prompt
uploaded = files.upload()

# STEP 2: Dynamically get the filename from upload dictionary
filename = list(uploaded.keys())[0]

# STEP 3: Load CSV into DataFrame
df = pd.read_csv(io.BytesIO(uploaded[filename]))

# STEP 4: Quick Data Overview
print(df.head())

print(df.info())

print(df.describe())



"""*Missing Values & Duplicates Check*"""

# Check for missing values
print("Missing Values per Column:")
print(df.isnull().sum())

# Check percentage of missing data
missing_percent = df.isnull().mean() * 100
print("\nPercentage Missing:")
print(missing_percent)

# Check for duplicates
print("\nTotal duplicate rows:", df.duplicated().sum())

"""*Data Types & Standardization Check*"""

# Check data types
print("\nData Types:")
print(df.dtypes)

# Quick check for unique values in key columns (standardization check)
if 'region' in df.columns:
    print("\nUnique regions:", df['region'].unique())
if 'product_category' in df.columns:
    print("\nUnique product categories:", df['product_category'].unique())
if 'product_name' in df.columns:
    print("\nSample product names:", df['product_name'].unique()[:10])

"""*Standardize & Rename Columns (if needed)*"""

# Optional renaming for clarity
df.rename(columns={'Lead times': 'Supplier Lead Time',
                   'Lead time': 'Manufacturing Lead Time'}, inplace=True)

"""*Scaling / Normalization*"""

from sklearn.preprocessing import StandardScaler

# 1. Select numerical columns for scaling
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# 2. Initialize scaler
scaler = StandardScaler()

# 3. Apply scaling
df_scaled = df.copy()
df_scaled[num_cols] = scaler.fit_transform(df[num_cols])

print("\nScaled Numerical Features Sample:")
print(df_scaled[num_cols].head())

# Optional: Keep original df intact, work with df_scaled going forward

""" *Categorical Encoding*"""

from sklearn.preprocessing import OneHotEncoder

# 1. Identify categorical columns
cat_cols = df_scaled.select_dtypes(include=['object']).columns
print("Categorical Columns:", cat_cols)

# 2. Apply One-Hot Encoding
df_encoded = pd.get_dummies(df_scaled, columns=cat_cols, drop_first=True)

print("\nEncoded DataFrame Shape:", df_encoded.shape)
print(df_encoded.head())

"""*optional new features*"""

# Example of engineered features
df_encoded['Revenue_per_Unit'] = df['Revenue generated'] / df['Number of products sold']
df_encoded['Shipping_Cost_per_Product'] = df['Shipping costs'] / df['Number of products sold']
df_encoded['Defects_per_1000_units'] = df['Defect rates'] * 1000

print("\nEngineered Feature Samples:")
print(df_encoded[['Revenue_per_Unit', 'Shipping_Cost_per_Product', 'Defects_per_1000_units']].head())

"""*Full EDA with Graphs*"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns

# Make sure plots are visible inside the notebook
# %matplotlib inline
sns.set(style="whitegrid", palette="pastel")

# --- 1. Product Sales Distribution ---
plt.figure(figsize=(8, 5))
sns.histplot(df['Number of products sold'], bins=30, kde=True)
plt.title('Distribution of Products Sold')
plt.xlabel('Number of Products Sold')
plt.ylabel('Frequency')
plt.show()

"""Product Sales Distribution:

The sales distribution looks right-skewed, but there are products with both low and very high sales, showing possible fast-movers and slow-movers.
This will be critical for demand segmentation later.
"""

# --- 2. Revenue vs Costs Scatter ---
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='Revenue generated', y='Costs', hue='Product type')
plt.title('Revenue vs Supply Chain Costs by Product Type')
plt.xlabel('Revenue')
plt.ylabel('Total Supply Chain Costs')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""Revenue vs Supply Chain Costs by Product Type:

Some high-revenue products still have high supply chain costs, especially across all 3 categories (haircare, skincare, cosmetics).
Might be worth digging into product-specific margin optimization.
"""

# --- 3. Defect Rates by Supplier ---
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Supplier name', y='Defect rates')
plt.title('Defect Rates per Supplier')
plt.xticks(rotation=90)
plt.ylabel('Defect Rate')
plt.show()

"""Defect Rates per Supplier:

Suppliers like Supplier 5 and Supplier 3 have a wider defect rate range (up to ~5%).
We could recommend actions like supplier audits or quality scorecards.

"""

# --- 4. Stock Levels vs Region (if location = region) ---
if 'Location' in df.columns:
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df, x='Location', y='Stock levels', estimator=np.sum)
    plt.title('Total Stock Levels per Location')
    plt.xticks(rotation=45)
    plt.ylabel('Total Stock')
    plt.show()

"""Stock Levels per Location:

Kolkata has noticeably higher stock levels compared to other cities like Delhi and Chennai.
Potential inventory imbalance → suggests excess holding costs or demand mismatch in Kolkata.
"""

# --- 5. Correlation Heatmap ---
plt.figure(figsize=(12, 8))
corr = df.select_dtypes(include=['int64', 'float64']).corr()
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""Correlation Heatmap:

Weak-to-moderate correlation overall.
Manufacturing lead time is negatively correlated with Price.
Costs show some mild correlation with Supplier Lead Time (0.24) → longer lead times may be driving costs up slightly.

Diagnostic Insights (Early Ideas):

*   You might have inventory inefficiencies by region.
*   Certain suppliers could be inflating defect rates, impacting returns or customer satisfaction.
*   Some high-revenue products may still be eroding margins due to elevated supply chain costs.

*Additional Supply Chain Insights*

Shipping Time vs Supplier
"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Supplier name', y='Shipping times')
plt.title('Shipping Time per Supplier')
plt.ylabel('Shipping Time (Days)')
plt.xticks(rotation=45)
plt.show()



"""Shipping Time per Supplier:


*   Supplier 1 & Supplier 5 tend to have longer median shipping times (~7-8 days).


*   Supplier 3 and Supplier 4 show more variability but with shorter median times (~4-6 days).

*   Actionable Insight: We could flag Supplier 1 & 5 as riskier for lead time variability and late deliveries → good candidates for renegotiation or dual sourcing.

2. Shipping Costs by Transportation Mode
"""

plt.figure(figsize=(8, 5))
sns.barplot(data=df, x='Transportation modes', y='Shipping costs', estimator=np.mean)
plt.title('Average Shipping Cost by Transportation Mode')
plt.ylabel('Avg Shipping Cost')
plt.show()

"""Shipping Costs by Transportation Mode:

*   Air freight is the most expensive (no surprise!).
*   Sea shipping has the lowest average cost.

*   Insight: For non-urgent products, you might shift some volume from air to sea

Pareto Chart (80/20 rule on Costs by Product)
"""

# Calculate cumulative % of total costs by SKU
costs_by_sku = df.groupby('SKU')['Costs'].sum().sort_values(ascending=False)
cumulative_cost_pct = costs_by_sku.cumsum() / costs_by_sku.sum() * 100

plt.figure(figsize=(10, 5))
ax = costs_by_sku.plot(kind='bar')
cumulative_cost_pct.plot(secondary_y=True, color='red', marker='o', ax=ax)

plt.title('Pareto Chart: Costs by SKU')
ax.set_ylabel('Total Costs')
plt.ylabel('% of Cumulative Cost', color='red')
plt.grid(True)
plt.show()

"""Pareto Chart (80/20 Rule on SKU Costs):

*   Classic Pareto curve
*   ~20% of SKUs account for about 80%+ of total costs.

*   Insight: Focus demand forecasting, inventory optimization, and cost-saving strategies on this critical 20% → it’s your high-impact SKU cluster.

Why these are important:

*   Supplier-Shipping analysis → pinpoint which suppliers might be causing delivery delays.
*   Transport mode costing → decide where to switch from air freight to ground transport, etc.

*   Pareto chart → spot the small % of SKUs causing the largest % of costs (classic inventory & cost optimization starting point).

*Advanced Diagnostics*

Transport Cost vs Region & Mode
"""

plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='Location', y='Shipping costs', hue='Transportation modes', estimator=np.mean)
plt.title('Avg Shipping Cost by Region & Transport Mode')
plt.ylabel('Average Shipping Cost')
plt.xticks(rotation=45)
plt.legend(title="Transport Mode", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()



"""Transport Cost by Region & Mode:


*   Mumbai and Bangalore are relying heavily on air freight with higher average costs compared to other modes like sea and rail.
*   Kolkata has surprisingly lower sea shipping costs compared to other cities.


*   Chennai and Delhi show some balance but still lean toward air and road for some lanes.

👉 Insight:
*   Recommend shifting Mumbai & Bangalore's non-urgent shipments from air to sea/rail to reduce costs.

*   Kolkata seems to benefit from better sea freight rates — could be leveraged further.

*Supplier Performance Matrix (Defect Rate + Shipping Delay Combined)*
"""

# Calculate average defect rate and shipping time per supplier
supplier_perf = df.groupby('Supplier name').agg({
    'Defect rates': 'mean',
    'Shipping times': 'mean'
}).reset_index()

# Scatter plot: Defect Rate vs Shipping Time
plt.figure(figsize=(8, 5))
sns.scatterplot(data=supplier_perf, x='Shipping times', y='Defect rates', hue='Supplier name', s=100)
plt.title('Supplier Performance: Defect Rate vs Shipping Delay')
plt.xlabel('Avg Shipping Time (Days)')
plt.ylabel('Avg Defect Rate (%)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()

"""Supplier Risk Matrix (Defects + Shipping Delay):

*   Supplier 5 is clearly the most concerning with the highest shipping time and highest defect rate (~6.2 days and ~2.7%).

*   Supplier 1 has the lowest defect rate (1.8%) and moderate shipping time (6.1 days).


*   Supplier 3 seems balanced but could still improve on defects.

👉 Insight:

*   Consider Supplier 5 as a priority for review or performance improvement initiatives.
*   Supplier 1 could be a benchmark supplier (low defects, stable shipping times).

**Supplier Risk Score**
"""

from sklearn.preprocessing import MinMaxScaler

# Prepare supplier performance table
supplier_perf = df.groupby('Supplier name').agg({
    'Defect rates': 'mean',
    'Shipping times': 'mean'
}).reset_index()

# Scale both metrics between 0 and 1
scaler = MinMaxScaler()
supplier_perf[['Defect_Scaled', 'Shipping_Scaled']] = scaler.fit_transform(
    supplier_perf[['Defect rates', 'Shipping times']]
)

# Combine into a simple weighted risk score (equal weights for now)
supplier_perf['Risk_Score'] = 0.5 * supplier_perf['Defect_Scaled'] + 0.5 * supplier_perf['Shipping_Scaled']

# Sort by highest risk
supplier_perf = supplier_perf.sort_values('Risk_Score', ascending=False)

print(supplier_perf[['Supplier name', 'Defect rates', 'Shipping times', 'Risk_Score']])

"""🎯 Interpretation of Your Supplier Risk Score:

🚩 Supplier 5 = highest risk (score = 1.00) → worst combo of long shipping delays + high defect rates.

Supplier 4 & Supplier 2 = moderate risk, but still above average.

✅ Supplier 3 = your safest bet (lowest score at 0.38) → solid shipping and decent quality.

⚡ Supply Chain Actionable Moves:

Supplier 5 should be flagged for immediate risk mitigation:
Negotiate better SLAs.

Diversify sourcing (maybe shift some demand to Supplier 3 or Supplier 1).

Use Supplier 3 as a best-practice benchmark

**Heatmap for Supplier Risk Score**
"""

plt.figure(figsize=(6, 4))
sns.heatmap(supplier_perf.set_index('Supplier name')[['Risk_Score']], annot=True, cmap="Reds", cbar=True, fmt=".2f")
plt.title('Supplier Risk Score Heatmap')
plt.ylabel('Supplier')
plt.xlabel('Risk Score')
plt.show()

"""🟥 Heatmap takeaway:

Supplier 5 🔴 = clearly flagged as the biggest supply chain risk.

The gradation in red shades immediately shows who’s safe (lighter colors) vs risky (deep red).

Radar Chart (Spider Plot) for Supplier KPIs
"""

# Radar Chart Prep
import numpy as np

# KPIs to plot
categories = ['Defect rates', 'Shipping times', 'Risk_Score']
N = len(categories)

# Create angle values for radar
angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]  # close the circle

# Radar plot for each supplier
plt.figure(figsize=(6,6))
for i, row in supplier_perf.iterrows():
    values = row[categories].tolist()
    values += values[:1]
    plt.polar(angles, values, label=row['Supplier name'], marker='o')

plt.xticks(angles[:-1], categories)
plt.title('Supplier Performance Radar Chart')
plt.legend(bbox_to_anchor=(1.2, 1.0))
plt.show()

"""🕸️ Radar chart insight:

Supplier 5 is sticking out across all dimensions (risk score, defects, shipping times).

Supplier 3 is "hugging the center" = low-risk, consistent performer.

Super handy for supplier comparison at-a-glance!
"""



"""**ML Demand Prediction Model**

Goal:

Predict "Number of products sold" (demand) based on:


Product info (price, category)

Region (location)

Inventory & supply chain factors (stock levels, lead times, costs, etc.)
"""



"""*Data Prep (Target + Features)*"""

# ✅ STEP 1: Set target variable
target = 'Number of products sold'

# ✅ STEP 2: Dynamically detect and drop all 'SKU_' columns (since they're now dummies)
sku_cols = [col for col in df_encoded.columns if col.startswith('SKU_')]

# ✅ STEP 3: Drop 'Revenue generated', target & SKU dummies
X = df_encoded.drop(columns=[target, 'Revenue generated'] + sku_cols)
y = df_encoded[target]

print("X shape:", X.shape)
print("y shape:", y.shape)

# ✅ STEP 4: Train/Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training samples:", X_train.shape[0])
print("Test samples:", X_test.shape[0])

# ✅ STEP 5: Train Random Forest Model
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ✅ STEP 6: Make Predictions & Evaluate
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"\n🎯 RMSE: {rmse:.2f}")
print(f"🎯 R² Score: {r2:.2%}")



"""XGBoost + Hyperparameter Tuning"""

!pip install xgboost

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV

# Initialize basic XGBoost
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)

# Hyperparameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5]
}

# Randomized Search
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=25,
    cv=3,
    scoring='r2',
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Fit to training data
random_search.fit(X_train, y_train)

# Best model
best_xgb = random_search.best_estimator_

# Predict & Evaluate
y_pred_xgb = best_xgb.predict(X_test)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)

print("\n🔥 Tuned XGBoost Results 🔥")
print(f"RMSE: {rmse_xgb:.2f}")
print(f"R² Score: {r2_xgb:.2%}")
print(f"Best Parameters: {random_search.best_params_}")

"""Unexpected Outcome:

R² dropped slightly .


RMSE also increased a bit (0.46) from 0.43.

⚠️ Why?

This sometimes happens when XGBoost over-regular
izes or overfits during tuning depending on your data characteristics.

Your dataset might already be well-handled by Random Forest, especially since Random Forest tends to handle noisy categorical-heavy datasets better "out of the box" without much tuning

✅ Let’s go for Feature Engineering Phase 2 to introduce new domain-specific features.
This will help both models improve and also give you more supply chain intelligence.
"""



"""*Advanced Feature Engineering Ideas*"""

df_fe = df.copy()  # start from original data before scaling/encoding

# 🔵 1. Shipping Cost Per Unit Sold
df_fe['Shipping_Cost_per_Unit'] = df_fe['Shipping costs'] / df_fe['Number of products sold'].replace(0, 1)

# 🔵 2. Revenue Per Product
df_fe['Revenue_per_Product'] = df_fe['Revenue generated'] / df_fe['Number of products sold'].replace(0, 1)

# 🔵 3. Stock Cover Ratio (Stock levels relative to sales)
df_fe['Stock_Cover_Ratio'] = df_fe['Stock levels'] / df_fe['Number of products sold'].replace(0, 1)

# 🔵 4. Lead Time Pressure (Supplier Lead Time * Order Quantity)
df_fe['Lead_Time_Pressure'] = df_fe['Supplier Lead Time'] * df_fe['Order quantities']

# 🔵 5. Cost Efficiency (Total Costs / Revenue)
df_fe['Cost_Efficiency'] = df_fe['Costs'] / df_fe['Revenue generated'].replace(0, 1)

# Check new features
print(df_fe[['Shipping_Cost_per_Unit', 'Revenue_per_Product', 'Stock_Cover_Ratio',
             'Lead_Time_Pressure', 'Cost_Efficiency']].head())

"""Here’s what you just created:

Shipping_Cost_per_Unit: Helps capture inefficiencies in logistics.

Revenue_per_Product: Spot high-margin vs low-margin products.

Stock_Cover_Ratio: Indicates potential overstock or stockout risk.

Lead_Time_Pressure: Combines lead time & order volume — perfect for supply chain bottleneck detection.

Cost_Efficiency: How lean each transaction is relative to revenue.

*Encode + Scale with New Features*
"""

# Drop or merge these engineered features back into df_encoded
df_encoded = df_encoded.merge(
    df_fe[['Shipping_Cost_per_Unit', 'Revenue_per_Product', 'Stock_Cover_Ratio',
           'Lead_Time_Pressure', 'Cost_Efficiency']],
    left_index=True, right_index=True
)

# Optional: Drop any previous engineered feature overlaps if needed

"""*Retrain Random Forest + XGBoost next*"""

# Redefine X and y
X = df_encoded.drop(columns=[target, 'Revenue generated'] + sku_cols)
y = df_encoded[target]

# Train/Test Split again
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Retrain Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict
y_pred_rf = rf_model.predict(X_test)

# Evaluate RF
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("🌲 Random Forest Results 🌲")
print(f"RMSE: {rmse_rf:.2f}")
print(f"R² Score: {r2_rf:.2%}")

"""Retrain XGBoost (using best params from earlier)"""

from xgboost import XGBRegressor

# Use the best parameters you got earlier
xgb_model = XGBRegressor(
    objective='reg:squarederror',
    subsample=0.8,
    n_estimators=300,
    max_depth=7,
    learning_rate=0.1,
    gamma=1,
    colsample_bytree=0.8,
    random_state=42
)

# Fit XGBoost
xgb_model.fit(X_train, y_train)

# Predict
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate XGBoost
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)

print("\n⚡ XGBoost Results ⚡")
print(f"RMSE: {rmse_xgb:.2f}")
print(f"R² Score: {r2_xgb:.2%}")

"""🎉 Major Improvement Unlocked:

Random Forest:

✅ R² = 86.83% (improved from ~82%)

✅ RMSE dropped to 0.37

XGBoost:

✅ R² = 87.09% (better than RF now!)

✅ RMSE = 0.37
"""



"""*Feature Importance Plot (XGBoost)*"""

import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from XGBoost model
importances = xgb_model.feature_importances_
features = X_train.columns

# Create dataframe
feat_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=feat_importance_df.head(15), x='Importance', y='Feature', palette='Blues_d')
plt.title(' Top 15 Feature Importances (XGBoost)')
plt.xlabel('Relative Importance')
plt.ylabel('Feature')
plt.show()

"""🎯 Top Insights from Your Feature Importance:

Shipping_Cost_per_Unit & Shipping_Cost_per_Product are the top drivers!

➡️ This shows how much logistics inefficiencies are influencing demand patterns (e.g., products with high unit shipping costs may be harder to push in certain regions).


Revenue_per_Product & Stock_Cover_Ratio

➡️ Tells you that margins + inventory positioning play a major role in how much demand you're actually fulfilling.

Lead_Time_Pressure & Manufacturing costs

➡️ Operational stress factors are contributing to supply-demand dynamics (e.g., higher lead time x order volumes = forecasting bottlenecks).

🚦 Big takeaway:

Your demand is highly tied to logistics unit costs and inventory efficiency rather than just traditional factors like price or stock alone.

Random Forest Feature Importance Plot
"""

# Get feature importances from Random Forest model
rf_importances = rf_model.feature_importances_

# Create dataframe
rf_feat_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_importances
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=rf_feat_importance_df.head(15), x='Importance', y='Feature', palette='Greens_d')
plt.title(' Top 15 Feature Importances (Random Forest)')
plt.xlabel('Relative Importance')
plt.ylabel('Feature')
plt.show()

"""Quick takeaway from comparison:


*   Both XGBoost and Random Forest agree that shipping cost per unit/product + stock cover ratio are your top supply chain demand drivers.
*   RF leans slightly heavier on "Shipping_Cost_per_Product", while XGBoost spreads influence more evenly across operational stressors like lead time pressure and manufacturing costs.





"""



"""**Step 6: Simulation & Optimization**

Inventory & Replenishment Optimization

⚙️ Plan:

Simulation:

We can model stock levels vs forecasted demand in Python to identify supply imbalances.

Optimization Model (CPLEX):

Formulate a Mixed Integer Programming (MIP) model:

Objective = Minimize total supply chain cost

Decision variables = reorder quantities, stock levels

Constraints = demand satisfaction, warehouse capacity, min/max reorder limits.
"""



"""*Python Inventory Simulation*

We’ll simulate:

Forecasted demand vs current stock

Flag stockouts, overstock, and reorder triggers.

Prep input for your upcoming CPLEX optimizer.
"""

# Step 0: Create full dataset with a Forecast column placeholder
df_sim = df_fe.copy()
df_sim['Forecasted_Demand'] = np.nan

# Step 1: Fill in forecasts where index matches X_test
df_sim.loc[X_test.index, 'Forecasted_Demand'] = y_pred_xgb

# Step 2: For other rows (train set), we can optionally backfill with y_train or leave as NA for now
df_sim['Forecasted_Demand'] = df_sim['Forecasted_Demand'].fillna(df_sim['Number of products sold'])  # simple fill

# Step 3: Continue simulation logic...
df_sim['Stock_Surplus'] = df_sim['Stock levels'] - df_sim['Forecasted_Demand']

df_sim['Stockout_Flag'] = df_sim['Stock_Surplus'] < 0
df_sim['Overstock_Flag'] = df_sim['Stock_Surplus'] > df_sim['Forecasted_Demand'] * reorder_point_factor

df_sim['Holding_Cost'] = df_sim[['Stock_Surplus', 'Forecasted_Demand']].apply(
    lambda row: holding_cost_per_unit * max(0, row['Stock_Surplus']), axis=1
)
df_sim['Stockout_Cost'] = df_sim[['Stock_Surplus']].apply(
    lambda row: stockout_cost_per_unit * abs(min(0, row['Stock_Surplus'])), axis=1
)

df_sim['Total_Simulation_Cost'] = df_sim['Holding_Cost'] + df_sim['Stockout_Cost']

# Report
print(df_sim[['Location', 'Product type', 'Forecasted_Demand', 'Stock levels',
              'Stock_Surplus', 'Stockout_Flag', 'Overstock_Flag', 'Total_Simulation_Cost']].head())

"""🎯 Insights from your simulation output:

Severe stockouts detected:


Example:

* 736 units predicted demand vs 53 stock in Mumbai skincare → huge gap triggering a ₹6,830 stockout cost.

* Multiple stockouts across regions and product types.

Overstock alerts:

* Haircare in Mumbai shows high surplus stock with holding costs.

What this means for your supply chain:

* You have an imbalance of excess inventory in some locations while critical shortages exist elsewhere.

* Now we can optimize reallocation or replenishment plans to reduce this total simulated cost (holding + stockout).

*Visual Diagnostics*
"""

plt.figure(figsize=(8, 5))
stockout_matrix = df_sim[df_sim['Stockout_Flag']].pivot_table(
    index='Location', columns='Product type', values='Stockout_Flag', aggfunc='sum', fill_value=0
)

sns.heatmap(stockout_matrix, annot=True, cmap="Reds", linewidths=0.5)
plt.title("🔥 Stockouts Heatmap by Region & Product Type")
plt.xlabel("Product Type")
plt.ylabel("Region")
plt.show()

"""*Total Stockout Cost by Region*"""

plt.figure(figsize=(8, 5))
region_costs = df_sim.groupby('Location')['Stockout_Cost'].sum().sort_values(ascending=False)
sns.barplot(x=region_costs.index, y=region_costs.values, palette="Reds_r")
plt.title("Total Stockout Cost per Region")
plt.ylabel("Total Stockout Cost")
plt.xlabel("Region")
plt.show()

"""📝 What the diagnostics reveal:

Heatmap Insights:
* Kolkata skincare has the most severe stockout issue (11 stockouts) → major red flag.
* Mumbai cosmetics and Chennai skincare are also trouble zones.
You’ve got wide-spread shortages across multiple categories.

Stockout Cost by Region:

* Kolkata is leading with the highest stockout cost, followed closely by Mumbai and Delhi.
* Bangalore is doing relatively better but still contributes to the overall loss.

🎯 Optimization Focus Points:

* High-priority cities for inventory redistribution = Kolkata, Mumbai, and Delhi.

* Product types like skincare in Kolkata and cosmetics in Mumbai need special attention.

*Inventory Optimization using CPLEX*

Pre-req in Colab (CPLEX setup):
"""

!pip install cplex
!pip install docplex

"""Formulate the model"""

from docplex.mp.model import Model

# Create model
mdl = Model(name="Inventory_Optimization")

# Decision variables: How much to reorder for each region-product combo
reorder_vars = {}
for idx, row in df_sim.iterrows():
    key = (row['Location'], row['Product type'], idx)
    reorder_vars[key] = mdl.continuous_var(name=f"reorder_{key}")

# Objective function: Minimize total cost
total_cost = mdl.sum(
    holding_cost_per_unit * mdl.max(0, row['Stock levels'] + reorder_vars[(row['Location'], row['Product type'], idx)] - row['Forecasted_Demand']) +
    stockout_cost_per_unit * mdl.max(0, row['Forecasted_Demand'] - (row['Stock levels'] + reorder_vars[(row['Location'], row['Product type'], idx)]))
    for idx, row in df_sim.iterrows()
)

mdl.minimize(total_cost)

# Constraints (optional):
# For example, you can limit reorder quantity by max inventory or budget constraints if needed.

# Solve the model
solution = mdl.solve(log_output=True)

import pulp

# Create LP problem
prob = pulp.LpProblem("Inventory_Optimization", pulp.LpMinimize)

# Decision variables:
reorder_vars = {}
surplus_vars = {}
stockout_vars = {}

for idx, row in df_sim.iterrows():
    key = (row['Location'], row['Product type'], idx)

    # Reorder qty (continuous >= 0)
    reorder_vars[key] = pulp.LpVariable(f"reorder_{key}", lowBound=0, cat='Continuous')

    # Surplus >= 0
    surplus_vars[key] = pulp.LpVariable(f"surplus_{key}", lowBound=0, cat='Continuous')

    # Stockout >= 0
    stockout_vars[key] = pulp.LpVariable(f"stockout_{key}", lowBound=0, cat='Continuous')

# Linking constraints (surplus - stockout = stock + reorder - demand)
for idx, row in df_sim.iterrows():
    demand = row['Forecasted_Demand']
    stock = row['Stock levels']
    key = (row['Location'], row['Product type'], idx)
    reorder = reorder_vars[key]
    surplus = surplus_vars[key]
    stockout = stockout_vars[key]

    prob += surplus - stockout == stock + reorder - demand

# Objective function: Minimize total holding + stockout costs
total_cost = pulp.lpSum([
    holding_cost_per_unit * surplus_vars[key] + stockout_cost_per_unit * stockout_vars[key]
    for key in reorder_vars
])
prob += total_cost

# Solve using CBC solver
prob.solve(pulp.PULP_CBC_CMD(msg=True))

print("✅ Solver Status:", pulp.LpStatus[prob.status])
print("💰 Optimized Total Cost = ${:.2f}".format(pulp.value(prob.objective)))

# Extract solutions
df_sim['Optimal_Reorder'] = df_sim.index.map(lambda idx: reorder_vars[(df_sim.loc[idx, 'Location'], df_sim.loc[idx, 'Product type'], idx)].varValue)
df_sim['Post_Surplus'] = df_sim.index.map(lambda idx: surplus_vars[(df_sim.loc[idx, 'Location'], df_sim.loc[idx, 'Product type'], idx)].varValue)
df_sim['Post_Stockout'] = df_sim.index.map(lambda idx: stockout_vars[(df_sim.loc[idx, 'Location'], df_sim.loc[idx, 'Product type'], idx)].varValue)

"""Optimized Output Summary:

✅ Solver Status: Optimal → the model found the best solution based on your constraints.

💰 Total cost dropped to $1,948.88, way lower than your earlier simulation costs (which were in the tens of thousands).
"""



""" Impact Visualization"""

# Generate pivot table
post_stockout_matrix = df_sim[df_sim['Post_Stockout'] > 0].pivot_table(
    index='Location', columns='Product type', values='Post_Stockout', aggfunc='count', fill_value=0
)

# Plot only if there are any stockouts left
if post_stockout_matrix.values.sum() > 0:
    plt.figure(figsize=(8, 5))
    sns.heatmap(post_stockout_matrix, annot=True, cmap="Greens", linewidths=0.5)
    plt.title("✅ Post-Optimization Stockouts Heatmap")
    plt.xlabel("Product Type")
    plt.ylabel("Region")
    plt.show()
else:
    print("🎉 No post-optimization stockouts detected! Your network is fully balanced 🚀")

# Comparison summary
print(f"🚨 Pre-Optimization Stockouts: {df_sim['Stockout_Flag'].sum()}")
print(f"✅ Post-Optimization Stockouts: {(df_sim['Post_Stockout'] > 0).sum()}")



"""Pre vs Post Stockout Counts (Bar Chart)"""

plt.figure(figsize=(6,4))
pre_stockouts = df_sim['Stockout_Flag'].sum()
post_stockouts = (df_sim['Post_Stockout'] > 0).sum()

sns.barplot(x=['Pre-Optimization', 'Post-Optimization'], y=[pre_stockouts, post_stockouts], palette='viridis')
plt.title("📉 Stockouts Before vs After Optimization")
plt.ylabel("Number of Stockouts")
plt.show()

"""Pre vs Post Total Costs Breakdown"""

plt.figure(figsize=(6,4))

pre_cost = df_sim['Total_Simulation_Cost'].sum()
post_cost = pulp.value(prob.objective)

sns.barplot(x=['Pre-Optimization', 'Post-Optimization'], y=[pre_cost, post_cost], palette='Blues')
plt.title(" Total Cost Before vs After Optimization")
plt.ylabel("Total Cost ($)")
plt.show()

"""Optimal Reorder Quantities by Region"""

plt.figure(figsize=(8,5))
reorder_summary = df_sim.groupby('Location')['Optimal_Reorder'].sum()

sns.barplot(x=reorder_summary.index, y=reorder_summary.values, palette='mako')
plt.title("Optimal Reorder Quantities per Region")
plt.ylabel("Reorder Units")
plt.xlabel("Region")
plt.show()

"""1️⃣ Massive Stockout Elimination:

Pre-Optimization: ~79 stockouts!

Post-Optimization: 0 stockouts 🚀

➡️ You've achieved a fully balanced supply chain with zero unserved demand!

2️⃣ Huge Cost Reduction:

Pre-optimization supply chain cost = $320K+

Optimized cost = $1,900 💰

➡️ You’ve removed tens of thousands in stockout & holding penalties.

3️⃣ Smart Reorder Allocations:

Your optimizer now recommends higher replenishment for Kolkata, Delhi, and Mumbai, which were the hot zones for stockouts.

🎯 Business-ready takeaway:

"We reduced stockouts to zero and cut total supply chain costs by ~99% through optimal rebalancing and targeted reorder plans across regions."
"""

!pip install streamlit
!pip install pyngrok

""" *End-to-End Supply Chain Optimization Pipeline*"""

# ======================= CONFIG ==========================
run_eda = True
run_feature_engineering = True
run_forecasting = True
run_simulation = True
run_optimization = True
run_reporting = True

# =================== LIBRARY IMPORTS =====================
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error
import pulp
import logging
from datetime import datetime
import os
import platform
import pathlib
import shutil

# ==================== SETUP LOGGING ======================
logging.basicConfig(level=logging.INFO, format='🔹 %(message)s')

# ================== DATA INGESTION + UPLOAD HANDLER =======================
logging.info("📂 Upload or provide CSV path:")

try:
    # Google Colab upload
    from google.colab import files
    uploaded = files.upload()
    file_name = list(uploaded.keys())[0]
    df = pd.read_csv(file_name)
    logging.info(f"✅ Loaded file from Colab: {file_name}")

except:
    # Local Jupyter fallback
    file_name = input("🔵 Enter CSV file path (e.g., supply_chain_data.csv): ").strip()
    df = pd.read_csv(file_name)
    logging.info(f"✅ Loaded file from local path: {file_name}")

# ======================= EDA =============================
if run_eda:
    logging.info("Running EDA...")
    print(df.info())
    print(df.describe())
    sns.heatmap(df.isnull(), cbar=False)
    plt.title('Missing Data Heatmap')
    plt.show()

# ============= FEATURE ENGINEERING =======================
if run_feature_engineering:
    logging.info("Engineering Features...")
    df_fe = df.copy()
    supplier_lead_time_col = 'Supplier Lead Time' if 'Supplier Lead Time' in df_fe.columns else 'Lead times'
    df_fe['Shipping_Cost_per_Unit'] = df_fe['Shipping costs'] / df_fe['Number of products sold'].replace(0, 1)
    df_fe['Revenue_per_Product'] = df_fe['Revenue generated'] / df_fe['Number of products sold'].replace(0, 1)
    df_fe['Stock_Cover_Ratio'] = df_fe['Stock levels'] / df_fe['Number of products sold'].replace(0, 1)
    df_fe['Lead_Time_Pressure'] = df_fe[supplier_lead_time_col] * df_fe['Order quantities']
    df_fe['Cost_Efficiency'] = df_fe['Costs'] / df_fe['Revenue generated'].replace(0, 1)

# ================ DEMAND FORECASTING =====================
if run_forecasting:
    logging.info("Training XGBoost Demand Model...")
    df_encoded = pd.get_dummies(df_fe)
    target = 'Number of products sold'
    sku_cols = [col for col in df_encoded.columns if col.startswith('SKU_')]
    X = df_encoded.drop(columns=[target, 'Revenue generated'] + sku_cols)
    y = df_encoded[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = XGBRegressor(objective='reg:squarederror', n_estimators=300, max_depth=7, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    logging.info(f"R² Score: {r2_score(y_test, y_pred):.2%}")
    logging.info(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

# ================ INVENTORY SIMULATION ===================
if run_simulation:
    logging.info("Running Simulation...")
    df_sim = df_fe.copy()
    df_sim['Forecasted_Demand'] = np.nan
    df_sim.loc[X_test.index, 'Forecasted_Demand'] = y_pred
    df_sim['Forecasted_Demand'] = df_sim['Forecasted_Demand'].fillna(df_sim['Number of products sold'])
    holding_cost_per_unit = 2.0
    stockout_cost_per_unit = 10.0
    df_sim['Stock_Surplus'] = df_sim['Stock levels'] - df_sim['Forecasted_Demand']
    df_sim['Stockout_Flag'] = df_sim['Stock_Surplus'] < 0

# ================ OPTIMIZATION (PuLP) ===================
if run_optimization:
    logging.info("Running PuLP Optimizer...")
    prob = pulp.LpProblem("Inventory_Optimization", pulp.LpMinimize)
    reorder_vars, surplus_vars, stockout_vars = {}, {}, {}

    for idx, row in df_sim.iterrows():
        key = (row['Location'], row['Product type'], idx)
        reorder_vars[key] = pulp.LpVariable(f"reorder_{key}", lowBound=0)
        surplus_vars[key] = pulp.LpVariable(f"surplus_{key}", lowBound=0)
        stockout_vars[key] = pulp.LpVariable(f"stockout_{key}", lowBound=0)

    for idx, row in df_sim.iterrows():
        demand = row['Forecasted_Demand']
        stock = row['Stock levels']
        key = (row['Location'], row['Product type'], idx)
        prob += surplus_vars[key] - stockout_vars[key] == stock + reorder_vars[key] - demand

    prob += pulp.lpSum([holding_cost_per_unit * surplus_vars[k] + stockout_cost_per_unit * stockout_vars[k] for k in reorder_vars])
    prob.solve(pulp.PULP_CBC_CMD(msg=True))
    logging.info(f"Solver Status: {pulp.LpStatus[prob.status]}")
    logging.info(f"Optimized Cost: ${pulp.value(prob.objective):.2f}")

# ================ REPORTING + EXPORT ====================
if run_reporting:
    logging.info("Generating Final Reports...")
    df_sim['Optimal_Reorder'] = df_sim.index.map(lambda idx: reorder_vars[(df_sim.loc[idx, 'Location'], df_sim.loc[idx, 'Product type'], idx)].varValue)
    df_sim['Post_Stockout'] = df_sim['Stock levels'] + df_sim['Optimal_Reorder'] - df_sim['Forecasted_Demand']
    df_sim['Post_Stockout'] = df_sim['Post_Stockout'] < 0

    today = datetime.today().strftime('%Y-%m-%d')
    csv_file = f'optimized_inventory_{today}.csv'
    plot_file = f'stockout_reduction_chart_{today}.png'

    # ==== Create Chart ====
    pre_stockouts = df_sim['Stockout_Flag'].sum()
    post_stockouts = df_sim['Post_Stockout'].sum()

    plt.figure(figsize=(6,4))
    sns.barplot(x=['Pre-Optimization', 'Post-Optimization'],
                y=[pre_stockouts, post_stockouts],
                palette='viridis')
    plt.title("📉 Stockouts Before vs After Optimization")
    plt.ylabel("Number of Stockouts")
    plt.savefig(plot_file, bbox_inches='tight')
    plt.show()

    # ==== COLAB or LOCAL Handling ====
    try:
        # For Google Colab users (auto-download)
        from google.colab import files
        df_sim.to_csv(csv_file, index=False)
        files.download(csv_file)
        files.download(plot_file)
        logging.info("⬇️ Files downloaded via Colab!")
    except:
        # For local Jupyter / Desktop users (Downloads folder)
        def get_downloads_folder():
            if platform.system() in ["Windows", "Darwin"]:
                return str(pathlib.Path.home() / "Downloads")
            else:
                return os.getcwd()

        downloads_folder = get_downloads_folder()
        df_sim.to_csv(csv_file, index=False)
        shutil.move(csv_file, os.path.join(downloads_folder, csv_file))
        shutil.move(plot_file, os.path.join(downloads_folder, plot_file))
        logging.info(f"📥 Files saved to Downloads: {downloads_folder}")

    # Zero-stockout success message
    if post_stockouts == 0:
        logging.info("🎉 🎯 Supply Chain Optimized Successfully → ZERO stockouts remaining! 🚚✅")
    else:
        logging.info(f"⚠️ Remaining Stockouts After Optimization: {post_stockouts}")

"""Fully interactive Streamlit dashboard
* Real-time interactive dashboard for stakeholders to click through.
"""

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error
import pulp
import io

# =============== STREAMLIT CONFIG ====================
st.set_page_config(page_title="Supply Chain Optimization Dashboard", layout="wide")

# =============== SIDEBAR NAV ========================
st.sidebar.title("🔍 Dashboard Navigation")
page = st.sidebar.radio("Go to:", ["1️⃣ Upload Data", "2️⃣ Exploratory Analysis", "3️⃣ ML Forecasting", "4️⃣ Optimization & Reporting"])

# =============== FILE UPLOAD ========================
if page == "1️⃣ Upload Data":
    st.title("📂 Upload Supply Chain Data")
    uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.success("✅ File uploaded successfully!")
        st.dataframe(df.head())
        st.session_state.df = df

# =============== EDA ========================
elif page == "2️⃣ Exploratory Analysis":
    st.title("📊 Exploratory Data Analysis")
    if 'df' not in st.session_state:
        st.warning("⚠️ Please upload your dataset first.")
    else:
        df = st.session_state.df
        st.subheader("1️⃣ Data Overview")
        st.write(df.describe())
        st.subheader("2️⃣ Missing Values Heatmap")
        fig, ax = plt.subplots()
        sns.heatmap(df.isnull(), cbar=False, ax=ax)
        st.pyplot(fig)

# =============== ML FORECAST ========================
elif page == "3️⃣ ML Forecasting":
    st.title("🤖 Demand Forecasting with XGBoost")
    if 'df' not in st.session_state:
        st.warning("⚠️ Please upload your dataset first.")
    else:
        df = st.session_state.df.copy()
        supplier_lead_time_col = 'Supplier Lead Time' if 'Supplier Lead Time' in df.columns else 'Lead times'
        df['Shipping_Cost_per_Unit'] = df['Shipping costs'] / df['Number of products sold'].replace(0, 1)
        df['Revenue_per_Product'] = df['Revenue generated'] / df['Number of products sold'].replace(0, 1)
        df['Stock_Cover_Ratio'] = df['Stock levels'] / df['Number of products sold'].replace(0, 1)
        df['Lead_Time_Pressure'] = df[supplier_lead_time_col] * df['Order quantities']
        df['Cost_Efficiency'] = df['Costs'] / df['Revenue generated'].replace(0, 1)

        df_encoded = pd.get_dummies(df)
        target = 'Number of products sold'
        sku_cols = [col for col in df_encoded.columns if col.startswith('SKU_')]
        X = df_encoded.drop(columns=[target, 'Revenue generated'] + sku_cols)
        y = df_encoded[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = XGBRegressor(objective='reg:squarederror', n_estimators=300, max_depth=7, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        st.success(f"✅ R² Score: {r2_score(y_test, y_pred):.2%}")
        st.success(f"✅ RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

        # Save for later pages
        df['Forecasted_Demand'] = model.predict(X)
        st.session_state.df_model = df

# =============== OPTIMIZATION + REPORT ===============
elif page == "4️⃣ Optimization & Reporting":
    st.title("🚚 Inventory Optimization")

    if 'df_model' not in st.session_state:
        st.warning("⚠️ Please forecast demand first (run Step 3).")
    else:
        df = st.session_state.df_model.copy()

        # Pre-optimization flag
        df['Stock_Surplus'] = df['Stock levels'] - df['Forecasted_Demand']
        df['Stockout_Flag'] = df['Stock_Surplus'] < 0

        # PuLP Optimizer
        prob = pulp.LpProblem("Inventory_Optimization", pulp.LpMinimize)
        reorder_vars, surplus_vars, stockout_vars = {}, {}, {}
        holding_cost_per_unit = 2.0
        stockout_cost_per_unit = 10.0

        for idx, row in df.iterrows():
            reorder_vars[idx] = pulp.LpVariable(f"reorder_{idx}", lowBound=0)
            surplus_vars[idx] = pulp.LpVariable(f"surplus_{idx}", lowBound=0)
            stockout_vars[idx] = pulp.LpVariable(f"stockout_{idx}", lowBound=0)
            demand = row['Forecasted_Demand']
            stock = row['Stock levels']
            prob += surplus_vars[idx] - stockout_vars[idx] == stock + reorder_vars[idx] - demand

        prob += pulp.lpSum([holding_cost_per_unit * surplus_vars[k] + stockout_cost_per_unit * stockout_vars[k] for k in reorder_vars])
        prob.solve()

        # Results
        df['Optimal_Reorder'] = [reorder_vars[idx].varValue for idx in df.index]
        df['Post_Stockout'] = df['Stock levels'] + df['Optimal_Reorder'] - df['Forecasted_Demand']
        df['Post_Stockout'] = df['Post_Stockout'] < 0

        # Summary
        col1, col2 = st.columns(2)
        with col1:
            st.metric("📉 Pre-Optimization Stockouts", int(df['Stockout_Flag'].sum()))
        with col2:
            st.metric("✅ Post-Optimization Stockouts", int(df['Post_Stockout'].sum()))

        # Visualization
        st.subheader("📊 Stockouts Before vs After Optimization")
        fig, ax = plt.subplots()
        sns.barplot(x=['Pre-Optimization', 'Post-Optimization'],
                    y=[df['Stockout_Flag'].sum(), df['Post_Stockout'].sum()],
                    palette='viridis', ax=ax)
        st.pyplot(fig)

        # Download buttons
        st.subheader("⬇️ Download Optimized Inventory Plan")
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        st.download_button("Download CSV", csv_buffer.getvalue(), file_name="optimized_inventory.csv", mime="text/csv")

